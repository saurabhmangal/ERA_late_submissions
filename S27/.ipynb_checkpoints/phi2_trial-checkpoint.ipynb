{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50b4a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, AutoModel\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2741122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proxy_exported\n"
     ]
    }
   ],
   "source": [
    "### this is for running in local ###\n",
    "import os\n",
    "try:\n",
    "    os.environ['HTTP_PROXY']='http://185.46.212.90:80'\n",
    "    os.environ['HTTPS_PROXY']='http://185.46.212.90:80'\n",
    "    print (\"proxy_exported\")\n",
    "except:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "460f499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7758e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f86d46d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -U trl transformers accelerate peft einops datasets bitsandbytes scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68667ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Constants\n",
    "MODEL_NAME = \"microsoft/phi-2\"\n",
    "DATASET_NAME = \"OpenAssistant/oasst1\"\n",
    "OUTPUT_DIR = \"./results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5d47d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BitsAndBytes Config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "217bfe87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200eac28208241b5b481395bc633fd2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load Model and Tokenizer\n",
    "#model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb_config, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained('/home/saurabh/era_saurabh/late_submissions/s27/phi-2')\n",
    "model.config.use_cache = False\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8891ecc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiModel(\n",
      "  (embed_tokens): Embedding(51200, 2560)\n",
      "  (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x PhiDecoderLayer(\n",
      "      (self_attn): PhiAttention(\n",
      "        (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "        (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "        (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "        (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "        (rotary_emb): PhiRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): PhiMLP(\n",
      "        (activation_fn): NewGELUActivation()\n",
      "        (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "        (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "      )\n",
      "      (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "      (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print Model for Layer Identification\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40202749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LORA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"Wqkv\", \"out_proj\", \"fc1\", \"fc2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b27b6893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=700,\n",
    "    warmup_ratio=0.05,\n",
    "    lr_scheduler_type=\"constant\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5c749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Prepare Dataset\n",
    "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "assistant_responses = dataset.query('role == \"assistant\" and rank == 0.0')\n",
    "prompters = dataset.query('role == \"prompter\"').set_index(\"message_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5effbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Prompts and Responses\n",
    "assistant_responses['prompt_response'] = assistant_responses.apply(\n",
    "    lambda row: \"### Human: \" + prompters.loc[row.parent_id, 'text'] + \"### Assistant: \" + row['text'], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da30aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HuggingFace Dataset\n",
    "hf_dataset = datasets.Dataset.from_pandas(assistant_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d552a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=lora_config,\n",
    "    max_seq_length=32,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    dataset_text_field=\"text\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19b40f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust Normalization Layers\n",
    "for _, module in trainer.model.named_modules():\n",
    "    if isinstance(module, torch.nn.LayerNorm):\n",
    "        module.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b813e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075586cb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb826b2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Test the Model\n",
    "def generate_text(prompt):\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "    result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "    return result[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3248e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "print(generate_text(\"What is large language model?\"))\n",
    "print(generate_text(\"What is QLora that stands for Quantization and Low-Rank Adapters\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa134865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
