{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18a4bf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proxy_exported\n"
     ]
    }
   ],
   "source": [
    "### this is for running in local ###\n",
    "import os\n",
    "try:\n",
    "    os.environ['HTTP_PROXY']='http://185.46.212.90:80'\n",
    "    os.environ['HTTPS_PROXY']='http://185.46.212.90:80'\n",
    "    print (\"proxy_exported\")\n",
    "except:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f5e81f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70cdf387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1af9d58",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Huggingface datasets and tokenizers\n",
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f567cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef5181bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import get_config, get_weights_file_path\n",
    "from dataset import BilingualDataset, causal_mask\n",
    "from model import build_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5df56cd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "\n",
    "    decoder_input = torch.empty(1, 1,).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        #calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1,1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ee98ea5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples=2):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "\n",
    "    try:\n",
    "        # get the console window width\n",
    "        with os.popen('stty size', 'r') as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except:\n",
    "        console_width = 80\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
    "\n",
    "            assert encoder_input.size(0) == 1, \"batch size must be 1 for validation\"\n",
    "\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "            print (\"Count: \",count)\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
    "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
    "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
    "\n",
    "            if count == num_examples:\n",
    "                print_msg('-'*console_width)\n",
    "                break\n",
    "    if writer:\n",
    "        # Evaluate the character error rate\n",
    "        metric = torchmetrics.CharErrorRate()\n",
    "        cer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation cer', cer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        metric = torchmetrics.WordErrorRate()\n",
    "        wer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation wer', wer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        metric = torchmetrics.BLEUScore()\n",
    "        bleu = metric(predicted, expected)\n",
    "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9939da4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b35eda36",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6632e50c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_ds(config):\n",
    "#     if config['ds_loc'] == 'disk':\n",
    "#         ds_raw = load_from_disk(config['ds_path'])\n",
    "#     else:\n",
    "    ds_raw = load_dataset('opus_books', f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n",
    "\n",
    "    # Build Tokenizer\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config[\"lang_src\"])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config[\"lang_tgt\"])\n",
    "\n",
    "    # Keep 90% for training, 10% for validation\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "    sorted_train_ds = sorted(train_ds_raw, key= lambda x:len(x[\"translation\"][config[\"lang_src\"]]))\n",
    "    filtered_sorted_train_ds = [k for k in sorted_train_ds if len(k[\"translation\"][config[\"lang_src\"]]) < 150]\n",
    "    # filtered_sorted_train_ds = [k for k in filtered_sorted_train_ds if len(k[\"translation\"][config[\"lang_tgt\"]]) < 120]\n",
    "    filtered_sorted_train_ds = [k for k in filtered_sorted_train_ds if len(k[\"translation\"][config[\"lang_src\"]]) + 10 > len(k[\"translation\"][config[\"lang_tgt\"]])]\n",
    "\n",
    "    filtered_val_ds = [k for k in val_ds_raw if len(k[\"translation\"][config[\"lang_src\"]]) < 150]\n",
    "    filtered_val_ds = [k for k in filtered_val_ds if\n",
    "                                len(k[\"translation\"][config[\"lang_src\"]]) + 10 > len(\n",
    "                                    k[\"translation\"][config[\"lang_tgt\"]])]\n",
    "\n",
    "    train_ds = BilingualDataset(filtered_sorted_train_ds, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    val_ds = BilingualDataset(filtered_val_ds, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "\n",
    "    # Find the max len of each sentence in the source and target sentence\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "    max_len_src_sent = 0\n",
    "    max_len_tgt_sent = 0\n",
    "\n",
    "    for item in filtered_sorted_train_ds:\n",
    "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
    "        max_len_src_sent = max(max_len_src_sent, len(item['translation'][config['lang_src']]))\n",
    "        max_len_tgt_sent = max(max_len_tgt_sent, len(item['translation'][config['lang_tgt']]))\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f\"Max length of source sentence token: {max_len_src}\")\n",
    "    print(f\"Max length of target sentence token: {max_len_tgt}\")\n",
    "\n",
    "    print(f\"Max length of source sentence: {max_len_src_sent}\")\n",
    "    print(f\"Max length of target sentence: {max_len_tgt_sent}\")\n",
    "\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True, collate_fn=train_ds.collate_batch)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True, collate_fn=val_ds.collate_batch)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb048f1a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    encoder_input_max = max(x[\"encoder_str_length\"] for x in batch)\n",
    "    decoder_input_max = max(x[\"decoder_str_length\"] for x in batch)\n",
    "\n",
    "    encoder_inputs = []\n",
    "    decoder_inputs = []\n",
    "    encoder_mask = []\n",
    "    decoder_mask = []\n",
    "    label = []\n",
    "    src_text = []\n",
    "    tgt_text = []\n",
    "\n",
    "    for b in batch:\n",
    "        enc_input_tokens = b[\"encoder_input\"]  # Includes sos and eos\n",
    "        dec_input_tokens = b[\"decoder_input\"]\n",
    "\n",
    "        # Add sos, eos, padding to each sentence\n",
    "        enc_num_padding_tokens = encoder_input_max - len(enc_input_tokens)\n",
    "        dec_num_padding_tokens = decoder_input_max - len(dec_input_tokens)\n",
    "\n",
    "        # Check that number of tokens is positive\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too short\")\n",
    "\n",
    "        # only eos token for decoder output\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        encoder_inputs.append(b[\"encoder_input\"][:encoder_input_max])\n",
    "        decoder_inputs.append(b[\"encoder_input\"][:decoder_input_max])\n",
    "        encoder_mask.append((b[\"encoder_mask\"][0, 0, :encoder_input_max]).unsqueeze(0).unsqueeze(0).unsqueeze(0))\n",
    "        decoder_mask.append((b[\"decoder_mask\"][0, :decoder_input_max, :decoder_input_max]).unsqueeze(0).unsqueeze(0))\n",
    "        label.append(b[\"label\"][:decoder_input_max])\n",
    "        src_text.append(b[\"src_text\"])\n",
    "        tgt_text.append(b[\"tgt_text\"])\n",
    "    # print(len(encoder_inputs))\n",
    "    return  {\n",
    "        \"encoder_input\": torch.vstack(encoder_inputs),\n",
    "        \"decoder_input\": torch.vstack(decoder_inputs),\n",
    "        \"encoder_mask\": torch.vstack(encoder_mask),\n",
    "        \"decoder_mask\": torch.vstack(decoder_mask),\n",
    "        \"label\": torch.vstack(label),\n",
    "        \"src_text\": src_text,\n",
    "        \"tgt_text\": tgt_text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "337a03c7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config[\"seq_len\"], d_model=config[\"d_model\"], d_ff=config[\"d_ff\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c19200fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    # Define the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    Path(config[\"model_folder\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(config[\"experiment_name\"])\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "    STEPS_PER_EPOCH = len(train_dataloader)\n",
    "    MAX_LR = 10**-3\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                    max_lr=MAX_LR,\n",
    "                                                    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                                                    epochs=config[\"num_epochs\"],\n",
    "                                                    pct_start=0.2,\n",
    "                                                    div_factor=10,\n",
    "                                                    three_phase=True,\n",
    "                                                    final_div_factor=10,\n",
    "                                                    anneal_strategy=\"linear\"\n",
    "                                                    )\n",
    "\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    if config['preload']:\n",
    "        model_filename = get_weights_file_path(config, config['preload'])\n",
    "        print(f'Preloadng model {model_filename}')\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "        print(\"Preloaded\")\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1)\n",
    "\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            decoder_input = batch['decoder_input'].to(device)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)\n",
    "            decoder_mask = batch['decoder_mask'].to(device)\n",
    "\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "            proj_output = model.project(decoder_output) # (0, seq_len, vocab_size)\n",
    "\n",
    "            label = batch['label'].to(device)\n",
    "\n",
    "            # Compute loss using simple cross entropy\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Log the loss\n",
    "            writer.add_scalar('train_loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
    "            scheduler.step()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5db1210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Max length of source sentence token: 45\n",
      "Max length of target sentence token: 48\n",
      "Max length of source sentence: 149\n",
      "Max length of target sentence: 158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00:  73% 4470/6150 [03:46<01:25, 19.64it/s, loss=4.747]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 01: 100% 6150/6150 [05:08<00:00, 19.94it/s, loss=4.031]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"No,\" answered the reporter, \"a few bruises only from the ricochet!\n",
      "    TARGET: -- Non! répondit le reporter, quelques contusions seulement, par ricochet!\n",
      " PREDICTED: -- Non , répondit le reporter , un peu de quelques !\n",
      "Count:  2\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I had not been a week in my new office, when I happened to meet one evening a young Icoglan, extremely handsome and well-made.\n",
      "    TARGET: Je fus nommé pour aller servir d'aumônier à Constantinople auprès de monsieur l'ambassadeur de France.\n",
      " PREDICTED: Je n ' avais pas été un jour , quand je l ' ai été déjà jeune , quand je restai jeune , très jeune , très jeune .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 02:  12% 741/6150 [00:38<04:33, 19.78it/s, loss=3.655]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 03:  49% 3001/6150 [02:30<02:34, 20.39it/s, loss=3.920]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 04:  79% 4853/6150 [04:06<01:05, 19.84it/s, loss=3.774]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 05: 100% 6150/6150 [05:13<00:00, 19.61it/s, loss=2.978]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Having done all this I left them the next day, and went on board the ship.\n",
      "    TARGET: Ceci fait, je pris congé d'eux le jour suivant, et m'en allai à bord du navire.\n",
      " PREDICTED: Tout cela fait , je les ai abandonné le lendemain , et je me rendis à bord du navire .\n",
      "Count:  2\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Puis, levant les yeux sur notre héros, elle éclata de rire.\n",
      "    TARGET: Then, raising her eyes to our hero, she burst out laughing.\n",
      " PREDICTED: Then she burst into eyes , she not of our hero .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 06:  15% 908/6150 [00:45<04:25, 19.77it/s, loss=2.923]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 07:  45% 2768/6150 [02:21<02:47, 20.15it/s, loss=2.568]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 08:  78% 4781/6150 [04:02<01:10, 19.31it/s, loss=2.617]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 09: 100% 6150/6150 [05:13<00:00, 19.62it/s, loss=2.519]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: You will find here, moreover, the young woman of whom I spoke, who is persecuted, no doubt, in consequence of some court intrigue.\n",
      "    TARGET: Il y a plus, vous trouverez ici cette jeune femme persécutée sans doute par suite de quelque intrigue de cour.\n",
      " PREDICTED: Vous trouverez ici , d ’ ailleurs la jeune femme que je parlais , qui est sans doute , il y a quelque doute dans quelque sorte de la cour .\n",
      "Count:  2\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'This is the end of everything,' cried Madame de Renal, throwing herself into Julien's arms.\n",
      "    TARGET: – Voici la fin de tout, s’écria Mme de Rênal, en se jetant dans les bras de Julien.\n",
      " PREDICTED: Voilà tout ! s ’ écria Mme de Rênal en se jetant dans les bras de Julien .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 10:   9% 563/6150 [00:28<04:43, 19.68it/s, loss=2.076]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 11:  39% 2400/6150 [02:05<03:16, 19.08it/s, loss=1.878]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Processing Epoch 11: 100% 6150/6150 [05:18<00:00, 19.32it/s, loss=2.340]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Exquisite!\" Conseil replied.\n",
      "    TARGET: -- Exquis ! répondait Conseil.\n",
      " PREDICTED: -- ! répondit Conseil .\n",
      "Count:  2\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: But in matters of greater weight, I may suffer from want of money.\n",
      "    TARGET: Mais le manque de fortune peut m’exposer a des épreuves plus graves.\n",
      " PREDICTED: Mais dans les intérêts de la , je ne puis souffrir du reste .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 12: 100% 6150/6150 [05:15<00:00, 19.46it/s, loss=1.676]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: He pressed a metal button and at once the propeller slowed down significantly.\n",
      "    TARGET: Il pressa un bouton de métal, et aussitôt la vitesse de l'hélice fut très diminuée.\n",
      " PREDICTED: Il pressa un bouton de métal , et , il remit tout de suite , il his hélice .\n",
      "Count:  2\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: What were you doing at that window?\"\n",
      "    TARGET: Que faisiez-vous à cette fenêtre ? »\n",
      " PREDICTED: Que faites - vous donc à cette lucarne ?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 13: 100% 6150/6150 [05:13<00:00, 19.62it/s, loss=1.980]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It was the end of November and Holmes and I sat, upon a raw and foggy night, on either side of a blazing fire in our sitting-room in Baker Street.\n",
      "    TARGET: Fin novembre, Holmes et moi étions assis de chaque côté d’un bon feu dans notre petit salon de Baker Street ; dehors la nuit était rude, brumeuse.\n",
      " PREDICTED: C ’ était la fin du novembre et Holmes , assis sur une nuit noire , et nuit de côté , à nos pas de Baker Street , nos retour .\n",
      "Count:  2\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Charles was silent.\n",
      "    TARGET: Charles se taisait.\n",
      " PREDICTED: Charles se tut .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 14: 100% 6150/6150 [05:13<00:00, 19.61it/s, loss=1.784]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Go on!'\n",
      "    TARGET: Continuez ! »\n",
      " PREDICTED: Va ! »\n",
      "Count:  2\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Catherine bent forward and said in Étienne's ear:\n",
      "    TARGET: Catherine se pencha, dit a l'oreille d'Étienne:\n",
      " PREDICTED: Catherine s ' avança et dit a l ' oreille d ' Étienne .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 15: 100% 6150/6150 [05:16<00:00, 19.42it/s, loss=1.791]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: But I have not a minute to-day.\n",
      "    TARGET: Mais je n'ai pas le tempsaujourd'hui.\n",
      " PREDICTED: Mais je n ’ ai pas une minute aujourd ’ hui .\n",
      "Count:  2\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"One is the scullery-maid, who sleeps in the other wing.\n",
      "    TARGET: L’une est la laveuse de vaisselle, qui couche dans l’autre aile.\n",
      " PREDICTED: -- On est la fille de chambre de fille , qui dort à l ' autre .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 16: 100% 6150/6150 [05:14<00:00, 19.56it/s, loss=1.959]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Tomorrow, on the day stated and at the hour stated, the tide will peacefully lift it off, and it will resume its navigating through the seas.\"\n",
      "    TARGET: Demain, au jour dit, à l'heure dite, la marée le soulèvera paisiblement, et il reprendra sa navigation à travers les mers.\n",
      " PREDICTED: Demain , le jour , et , devant la heure , la marée paisiblement , s ' , et elle sa navigation à travers les mers .\n",
      "Count:  2\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"You will visit each of these in turn.\"\n",
      "    TARGET: – Vous les visiterez à tour de rôle.\n",
      " PREDICTED: -- Vous à propos de ce tour .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 17: 100% 6150/6150 [05:16<00:00, 19.41it/s, loss=1.723]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Let's forge ahead!\n",
      "    TARGET: Allons en avant !\n",
      " PREDICTED: Continuons la forge !\n",
      "Count:  2\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"And so have I, sir,\" I returned, putting my hands and my purse behind me. \"I could not spare the money on any account.\"\n",
      "    TARGET: -- Et moi aussi, monsieur, répondis-je en cachant ma bourse, je ne pourrais pas un instant me passer de cet argent.\n",
      " PREDICTED: -- Et moi aussi , monsieur , répliquai - je , en portant ma bourse derriere moi .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 18: 100% 6150/6150 [05:16<00:00, 19.41it/s, loss=1.928]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: A sort of uneasiness had seized Pencroft upon the subject of his vessel.\n",
      "    TARGET: Une sorte d'inquiétude avait pris Pencroff au sujet de son embarcation.\n",
      " PREDICTED: Une sorte de inquiétude avait donc à Pencroff de son compagnon .\n",
      "Count:  2\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Julien, without exactly knowing what he was doing, grasped her hand again.\n",
      "    TARGET: Julien, sans trop savoir ce qu’il faisait, la saisit de nouveau.\n",
      " PREDICTED: Julien , sans trop savoir ce qu ’ il faisait , lui prit la main des mains .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 19: 100% 6150/6150 [05:18<00:00, 19.33it/s, loss=1.842]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: — Bon, dit-il.\n",
      "    TARGET: \"That's good enough.\n",
      " PREDICTED: \" Good ,\" he said .\n",
      "Count:  2\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Doubtless,\" said Buckingham, \"and rather twice than once.\"\n",
      "    TARGET: -- Sans doute, dit Buckingham, et plutôt deux fois qu'une.\n",
      " PREDICTED: -- Sans doute , dit Buckingham , et un peu d ' ensemble .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 20: 100% 6150/6150 [05:15<00:00, 19.50it/s, loss=1.697]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Julien's attention was sharply distracted by the almost immediate arrival of a wholly different person.\n",
      "    TARGET: L’attention de Julien fut vivement distraite par l’arrivée presque immédiate d’un être tout différent.\n",
      " PREDICTED: L ’ attention de Julien fut très vivement du arrivée à l ’ arrivée d ’ une personne très différente .\n",
      "Count:  2\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"A good voyage to you,\" shouted the sailor, who himself did not expect any great result from this mode of correspondence.\n",
      "    TARGET: -- Bon voyage!» s'écria le marin, qui, lui, n'attendait pas grand résultat de ce mode de correspondance.\n",
      " PREDICTED: -- Une bonne traversée , fit le marin , qui ne s ' attendait guère à ce lieu de la haute en cet ordre .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 21: 100% 6150/6150 [05:16<00:00, 19.44it/s, loss=1.771]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Thousands of luminous sheets and barbed tongues of fire were cast in various directions.\n",
      "    TARGET: Des milliers de fragments lumineux et de points vifs se projetaient en directions contraires.\n",
      " PREDICTED: Un mille facettes de Sire lumineux et de morts furent à quelques instructions .\n",
      "Count:  2\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: In London he at last made acquaintance with the extremes of fatuity.\n",
      "    TARGET: À Londres, il connut enfin la haute fatuité.\n",
      " PREDICTED: Enfin il fit connaissance avec la connaissance des , en finissait par l ’ enfant .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 22: 100% 6150/6150 [05:15<00:00, 19.50it/s, loss=1.643]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Did you ever know one in your life that was transported and had a hundred pounds in his pocket, I'll warrant you, child?' says she.\n",
      "    TARGET: --Mais tu as de l'argent, n'est-ce pas? En as-tu déjà connu une dans ta vie qui se fît déporter avec 100£ dans sa poche?\n",
      " PREDICTED: - vous jamais dans votre vie de ce travail des et bien à cent livres ( je te l ’ dirai , dites - vous ?\n",
      "Count:  2\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Come, that I may tell you that very softly.\n",
      "    TARGET: « Venez, que je vous dise cela tout bas.\n",
      " PREDICTED: – Allons , que je puis vous dire cela très bas .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 23: 100% 6150/6150 [05:15<00:00, 19.50it/s, loss=1.854]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: He was afraid of bringing everything to an end by a sudden concession.\n",
      "    TARGET: Il avait peur de tout finir par une concession subite.\n",
      " PREDICTED: Il craignait de faire tout porter à une extrémité inférieure .\n",
      "Count:  2\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: They were now outside the forest, at the beginning of the powerful spurs which supported Mount Franklin towards the west.\n",
      "    TARGET: Il se trouvait en dehors de la forêt, à la naissance de ces puissants contreforts qui étançonnaient le mont Franklin vers l'est.\n",
      " PREDICTED: Ils étaient donc en dehors la forêt , au commencement des contreforts , que le mont Franklin donnait sur l ' ouest .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 24: 100% 6150/6150 [05:13<00:00, 19.61it/s, loss=1.699]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Their conduct has been such,\" replied Elizabeth, \"as neither you, nor I, nor anybody can ever forget.\n",
      "    TARGET: – Leur conduite a été telle, répliqua Elizabeth, que ni vous, ni moi, ni personne ne pourrons jamais l’oublier.\n",
      " PREDICTED: – Cette conduite a été d ’ autant , répondit Elizabeth , et je n ’ ai jamais rien d ’ oublier .\n",
      "Count:  2\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: There are lonely houses scattered over the moor, and he is a fellow who would stick at nothing.\n",
      "    TARGET: Il y a des maisons isolées sur la lande, et il ferait n’importe quoi.\n",
      " PREDICTED: Il y a des maisons qui sortent , et c ’ est un homme qui ne s ’ rien .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 25: 100% 6150/6150 [05:14<00:00, 19.58it/s, loss=1.991]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The old man shrugged his shoulders, and then let them fall as if overwhelmed beneath an avalanche of gold.\n",
      "    TARGET: Le vieux haussa les épaules, puis les laissa retomber, comme accablé sous un écroulement d'écus.\n",
      " PREDICTED: Le vieux eut un haussement d ' épaules , puis qu ' on se comme sous un de l ' or .\n",
      "Count:  2\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: There were reasons why I could not get there earlier.\"\n",
      "    TARGET: Voilà pourquoi je ne pouvais pas me rendre plus tôt au manoir.\n",
      " PREDICTED: J ’ ai des raisons que je ne pouvais voir là - bas .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 26: 100% 6150/6150 [05:13<00:00, 19.61it/s, loss=1.771]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"I pardon you, monseigneur!\" said Bonacieux, hesitating to take the purse, fearing, doubtless, that this pretended gift was but a pleasantry.\n",
      "    TARGET: -- Que je vous pardonne, Monseigneur! dit Bonacieux hésitant à prendre le sac, craignant sans doute que ce prétendu don ne fût qu'une plaisanterie.\n",
      " PREDICTED: -- Je vous le pardonne , Monseigneur , dit Bonacieux en sûre , sans crainte , que ce don ' t be une plaisanterie .\n",
      "Count:  2\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Jane, are you ready?\"\n",
      "    TARGET: -- Jane, êtes-vous prête?\n",
      " PREDICTED: « Jane , êtes - vous prêt ?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 27: 100% 6150/6150 [05:17<00:00, 19.37it/s, loss=1.574]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"A vessel from the Vineyard!\n",
      "    TARGET: «Un navire du Vineyard!\n",
      " PREDICTED: -- Un navire du Vineyard ?\n",
      "Count:  2\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: She repeated, \"He is out.\"\n",
      "    TARGET: Elle répéta: -- Il est absent.\n",
      " PREDICTED: Elle répéta : -- Il est absent .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 28: 100% 6150/6150 [05:18<00:00, 19.34it/s, loss=1.654]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: It was a bay horse hardly three years of age, called Trompette.\n",
      "    TARGET: C'était un cheval bai, de trois ans a peine, nommé Trompette.\n",
      " PREDICTED: C ' était un cheval de cinq ans , on ne l ' appelle Trompette .\n",
      "Count:  2\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Yes, but yesterday at five o’clock in the afternoon, thanks to you, she escaped.\"\n",
      "    TARGET: -- Oui, mais depuis hier cinq heures de l'après-midi, grâce à vous, elle s'est échappée.\n",
      " PREDICTED: -- Oui , mais hier à cinq heures du soir , merci , elle vous a sauvé .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 29: 100% 6150/6150 [05:20<00:00, 19.18it/s, loss=1.715]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  1\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Elizabeth disdained the appearance of noticing this civil reflection, but its meaning did not escape, nor was it likely to conciliate her.\n",
      "    TARGET: Elizabeth parut dédaigner cette réflexion aimable mais le sens ne lui en échappa point et, de plus en plus animée, elle reprit :\n",
      " PREDICTED: Elizabeth , qui s ’ en aperçut de cette réflexion n ’ avait pas dit , le ou l ’ on veut se passer de ce sur la premiere Darcy .\n",
      "Count:  2\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I remember my brother-in-law going for a short sea trip once, for the benefit of his health.\n",
      "    TARGET: Un jour, mon beau-frere fit une petite croisiere en mer, pour sa santé.\n",
      " PREDICTED: Je me rappelle mon beau - frère , avoir la mer pour le , car la santé de sa santé .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    cfg = get_config()\n",
    "    cfg['batch_size'] = 10\n",
    "    cfg['preload'] = 0\n",
    "    cfg['num_epochs'] = 30\n",
    "    train_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff87aa0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
