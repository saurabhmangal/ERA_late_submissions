{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16245dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4def438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ef487d2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Huggingface datasets and tokenizers\n",
    "from datasets import load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d0ebf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2de210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import get_config, get_weights_file_path\n",
    "from dataset import BilingualDataset, causal_mask\n",
    "from model import build_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1822a5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the current device\n",
    "torch.cuda.set_device(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c41f8833",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "\n",
    "    decoder_input = torch.empty(1, 1,).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        #calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.empty(1,1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbdf5204",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples=2):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "\n",
    "    try:\n",
    "        # get the console window width\n",
    "        with os.popen('stty size', 'r') as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except:\n",
    "        console_width = 80\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
    "\n",
    "            assert encoder_input.size(0) == 1, \"batch size must be 1 for validation\"\n",
    "\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
    "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
    "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
    "\n",
    "            if count == num_examples:\n",
    "                print_msg('-'*console_width)\n",
    "                break\n",
    "    if writer:\n",
    "        # Evaluate the character error rate\n",
    "        metric = torchmetrics.CharErrorRate()\n",
    "        cer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation cer', cer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        metric = torchmetrics.WordErrorRate()\n",
    "        wer = metric(predicted, expected)\n",
    "        writer.add_scalar('validation wer', wer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        metric = torchmetrics.BLEUScore()\n",
    "        bleu = metric(predicted, expected)\n",
    "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39594fe8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc498119",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9b19a5e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_ds(config):\n",
    "    if config['ds_loc'] == 'disk':\n",
    "        ds_raw = load_from_disk(config['ds_path'])\n",
    "    else:\n",
    "        ds_raw = load_dataset('opus_books', f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n",
    "\n",
    "    # Build Tokenizer\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config[\"lang_src\"])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config[\"lang_tgt\"])\n",
    "\n",
    "    # Keep 90% for training, 10% for validation\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "\n",
    "    # Find the max len of each sentence in the source and target sentence\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "\n",
    "    for item in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f\"Max length of source sentence: {max_len_src}\")\n",
    "    print(f\"Max length of target sentence: {max_len_tgt}\")\n",
    "\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46d309f2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config[\"seq_len\"], d_model=config[\"d_model\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bff79f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    # Define the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    Path(config[\"model_folder\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(config[\"experiment_name\"])\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    if config['preload']:\n",
    "        model_filename = get_weights_file_path(config, config['preload'])\n",
    "        print(f'Preloadng model {model_filename}')\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "        print(\"Preloaded\")\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1)\n",
    "\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            decoder_input = batch['decoder_input'].to(device)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)\n",
    "            decoder_mask = batch['decoder_mask'].to(device)\n",
    "\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "            proj_output = model.project(decoder_output) # (0, seq_len, vocab_size)\n",
    "\n",
    "            label = batch['label'].to(device)\n",
    "\n",
    "            # Compute loss using simple cross entropy\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Log the loss\n",
    "            writer.add_scalar('train_loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08aa5946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Max length of source sentence: 309\n",
      "Max length of target sentence: 274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00: 100% 4850/4850 [04:44<00:00, 17.06it/s, loss=6.510]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I was tormented by the contrast between my idea and my handiwork: in each case I had imagined something which I was quite powerless to realise.\"\n",
      "    TARGET: Anzi soffrivo per il contrasto che vi era fra l'ideale e l'opera e mi sentivo impotente a dar forma alle immagini della mia mente.\n",
      " PREDICTED: Non , e , e , e , e , e , e .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Now it's settled...'\n",
      "    TARGET: Ormai è concluso....\n",
      " PREDICTED: E che è un ’ ic .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 01: 100% 4850/4850 [04:47<00:00, 16.84it/s, loss=5.605]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Grimm has a fable called \"The Man Without a Shadow\" – about a man who lost his shadow as a punishment for something or other.\n",
      "    TARGET: C’è una favola di Grimm: l’uomo senza ombra, l’uomo privato dell’ombra. E questo gli è dato in castigo di qualcosa.\n",
      " PREDICTED: — , — disse , — e , — e , — e .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'Fürst Shcherbatsky sammt Gemahlin und Tochter,' [Prince Shcherbatsky with his wife and daughter.] by the premises they occupied, by their name, and by the people they were acquainted with, at once crystallized into their definite and preordained place.\n",
      "    TARGET: Fürst Šcerbackij sammt Gemahlin und Tochter per il nome e per l’appartamento che occupavano e per gli amici che avevano trovato, si cristallizzarono nel loro posto definito e ad essi destinato.\n",
      " PREDICTED: — , , , , e , , e , e , e , e , e , e e .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 02: 100% 4850/4850 [04:56<00:00, 16.37it/s, loss=3.982]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'Oh, nothing –' answered Oblonsky. 'We'll talk it over later on.\n",
      "    TARGET: — No, nulla — rispose Oblonskij. — Ne riparleremo.\n",
      " PREDICTED: — Oh , no — disse Stepan Arkad ’ ic .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I had, God knows, more sincerity than knowledge in all the methods I took for this poor creature’s instruction, and must acknowledge, what I believe all that act upon the same principle will find, that in laying things open to him, I really informed and instructed myself in many things that either I did not know or had not fully considered before, but which occurred naturally to my mind upon searching into them, for the information of this poor savage; and I had more affection in my inquiry after things upon this occasion than ever I felt before: so that, whether this poor wild wretch was better for me or no, I had great reason to be thankful that ever he came to me; my grief sat lighter, upon me; my habitation grew comfortable to me beyond measure: and when I reflected that in this solitary life which I have been confined to, I had not only been moved to look up to heaven myself, and to seek the Hand that had brought me here, but was now to be made an instrument, under Providence, to save the life, and, for aught I knew, the soul of a poor savage, and bring him to the true knowledge of religion and of the Christian doctrine, that he might know Christ Jesus, in whom is life eternal; I say, when I reflected upon all these things, a secret joy ran through every part of My soul, and I frequently rejoiced that ever I was brought to this place, which I had so often thought the most dreadful of all afflictions that could possibly have befallen me.\n",
      "    TARGET: In somma, sia o no divenuto migliore per opera mia quello sfortunato, certo ho grande motivo di ringraziare la celeste provvidenza che me lo inviò. I miei cordogli da quell’istante divennero più leggieri; la mia abitazione mi si rese oltremodo cara; e quando pensava che questo solitario confine mi fu non solo un impulso a volgere gli sguardi al cielo io medesimo e a cercare con affetto la mano che mi vi aveva condotto, ma era per rendermi con l’aiuto di Dio uno stromento alto a fare salva la vita e, a quanto sembrommi, l’anima di un povero selvaggio ed a condurlo su la via della religione e degl’insegnamenti della cristiana dottrina e dell’adorazione di Gesù Cristo in cui è la vita eterna: quando io pensava a tutto ciò, una segreta gioia comprendeva ogni parte della mia anima; e una tale idea frequentemente mi è stata di consolazione sino al termine del mio esilio in questo luogo: esilio ch’io aveva si spesso riguardato come la più spaventosa fra quante sventura avessero mai potuto avvenirmi.\n",
      " PREDICTED: Io mi era stato stato più di questo , che mi per , e io mi per me , e che mi , e che mi , e che mi , e che mi , e che mi , e che mi , e che mi , e che non mi , e che mi , e che mi , e che mi , e che mi , e che non mi mai più che mi , e non mi mai più che mi , e che non mai più che mi , e che mi , e che non mi mai più che non mi mai mai più che non mi mai che non mi mai mai mai che mi mai mai mai , e che mi , e che mi , e che mi , e che mi , e che mi , e che mi , e che mi mai , e che mi , e che mi , e che mi mai , e che mi , e che mi , e che mi , e che mi , e che mi , e che mi , e che mi , e che mi , e che mi , e che mi , e non , e che mi , e che mi , e non , e che mi mi mi mi mi mi mi , e che mi , e non , e che mi , e che mi , e non mi , e che mi , e che mi , e che mi mi mi mi , e , e ,\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 03: 100% 4850/4850 [04:49<00:00, 16.74it/s, loss=5.515]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: He lay awake half the night considering the details necessary for carrying his thought into effect.\n",
      "    TARGET: Non dormì per metà della notte, pensando ai particolari dell’esecuzione della sua idea.\n",
      " PREDICTED: Egli si era già già in una settimana in cui si era avvicinato .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Trying not to hurry and not to get excited, Levin gave the names of the doctor and of the midwife, explained why the opium was wanted and tried to persuade the dispenser to let him have it.\n",
      "    TARGET: Cercando di non avere fretta e di non accalorarsi, fatto il nome del dottore e quello della levatrice, e spiegato perché serviva l’oppio, Levin cominciò a persuaderlo.\n",
      " PREDICTED: Non si poteva andare e non si , Levin , e Levin , e la principessa , senza , si poteva e si poteva fare il dottore , e si poteva fare .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 04: 100% 4850/4850 [04:49<00:00, 16.78it/s, loss=4.426]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The river up to Sonning winds in and out through many islands, and is very placid, hushed, and lonely.\n",
      "    TARGET: Il fiume fino a Sonning serpeggia fra molte isole, ed è molto placido, raccolto e solitario.\n",
      " PREDICTED: Il fiume si avvicinò al fiume , e , dopo un poco , si , si , si e si .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'I shall still get angry with Ivan the coachman in the same way, shall dispute in the same way, shall inopportunely express my thoughts; there will still be a wall between my soul's holy of holies and other people; even my wife I shall still blame for my own fears and shall repent of it. My reason will still not understand why I pray, but I shall still pray, and my life, my whole life, independently of anything that may happen to me, is every moment of it no longer meaningless as it was before, but has an unquestionable meaning of goodness with which I have the power to invest it.'\n",
      "    TARGET: Mi arrabbierò sempre alla stessa maniera contro Ivan il cocchiere, sempre alla stessa maniera discuterò, esprimerò a sproposito le mie idee, ci sarà lo stesso muro fra il tempio dell’anima mia e quello degli altri, e perfino mia moglie accuserò sempre alla stessa maniera del mio spavento e ne proverò rimorso; sempre alla stessa maniera, non capirò con la ragione perché prego e intanto pregherò, ma la mia vita adesso, tutta la mia vita, indipendentemente da tutto quello che mi può accadere, ogni suo attimo, non solo non è più senza senso, come prima, ma ha un indubitabile senso di bene, che io ho il potere di trasfondere in essa!”.\n",
      " PREDICTED: — Io sono molto contento di andare in Russia , per il cocchiere , per il fratello , il mio desiderio , il mio desiderio , il mio desiderio , il mio desiderio , il mio desiderio , e per me ne sono un ’ altra , e per me ne , e non ci mai , ma non è possibile che il mio fratello non è possibile , ma che non è possibile , ma non è possibile , ma che non è possibile , ma non è possibile , ma non è possibile , ma che non è possibile .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 05: 100% 4850/4850 [04:52<00:00, 16.55it/s, loss=3.002]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Knitting, sewing, reading, writing, ciphering, will be all you will have to teach.\n",
      "    TARGET: Non dovete insegnar loro altro che a far la calza, a cucire, a leggere, a scrivere e a far di conto.\n",
      " PREDICTED: , , , tutti , tutto .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'I will fetch it at once.\n",
      "    TARGET: — La porto subito.\n",
      " PREDICTED: — Devo andare a casa .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 06: 100% 4850/4850 [04:45<00:00, 16.98it/s, loss=4.527]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"And you see the candles?\"\n",
      "    TARGET: — E vedete anche le candele?\n",
      " PREDICTED: — E voi avete visto la testa ?\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: After a while, however, less grasping feelings prevailed.\n",
      "    TARGET: Dopo un poco, però, prevalsero dei sentimenti meno esclusivi.\n",
      " PREDICTED: Dopo un certo modo , senza volere , si , e il suo corpo si .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 07: 100% 4850/4850 [04:50<00:00, 16.71it/s, loss=4.833]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Each time he began to think about it, he felt that he must try again, that by kindness, tenderness, and persuasion there was still a hope of saving her and obliging her to bethink herself. Every day he prepared himself to have a talk with her.\n",
      "    TARGET: Ogni qualvolta ci pensava, sentiva che era necessario tentare qualcosa, sentiva che, con la bontà, la tenerezza, la persuasione, c’era ancora la speranza di salvarla, di farla rientrare in sé, e ogni giorno si disponeva a parlare.\n",
      " PREDICTED: A ogni volta cominciò a pensare , cercando di capire che egli avrebbe provato , che la coscienza della disperazione , la coscienza della propria calma , e la sua calma , la tormentava , la voleva capire che cosa sarebbe stato un tempo di parlare .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'I like it,' said Anna.\n",
      "    TARGET: — No, mi piace — disse Anna.\n",
      " PREDICTED: — Io mi piace — disse Anna .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 08: 100% 4850/4850 [04:50<00:00, 16.72it/s, loss=4.062]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'But what has she done?' asked Levin rather indifferently. He wanted to consult her about his own affairs, and was annoyed at having come at an inopportune moment.\n",
      "    TARGET: — Ma che ha fatto mai? — chiese Levin alquanto indifferente, e, desideroso di trovar consiglio per la cosa sua, s’irritò d’esser capitato fuor di proposito.\n",
      " PREDICTED: — Ma come mai ha fatto ? — disse Levin con stizza . — Per favore , è necessario condurre la sua azienda , e , dopo aver cercato di non dormire , si in un momento di modo .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: She was not only glad to meet him, but searched his face for signs of the impression she created on him.\n",
      "    TARGET: Ella non solo gioiva di un incontro con lui, ma cercava sul viso di lui i segni dell’impressione che lei stessa produceva.\n",
      " PREDICTED: Non solo ella si sentiva in lui , ma in quel momento , nel suo viso , si sentiva completamente liberato del proprio mutamento di lui .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 09: 100% 4850/4850 [04:52<00:00, 16.56it/s, loss=3.191]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: You might look daggers at him for an hour and he would not notice it, and it would not trouble him if he did.\n",
      "    TARGET: Voi potevate lanciargli degli sguardi furiosi per un’ora, e lui non li vedeva, e non se ne sarebbe dato per inteso, se li avesse visti.\n",
      " PREDICTED: Se l ’ avesse guardato per un ’ ora , e lui non si sarebbe messo a guardarlo e se non lo avesse fatto .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: And that punishment you made me suffer because your wicked boy struck me--knocked me down for nothing.\n",
      "    TARGET: E quella punizione me l'avevate inflitta perché era stata percossa, gettata in terra dal vostro perfido figliuolo.\n",
      " PREDICTED: \" E mi che il vostro ragazzo mi temete per nulla .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 10: 100% 4850/4850 [04:45<00:00, 16.99it/s, loss=3.816]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'I don't think they play at all fairly,' Alice began, in rather a complaining tone, 'and they all quarrel so dreadfully one can't hear oneself speak--and they don't seem to have any rules in particular; at least, if there are, nobody attends to them--and you've no idea how confusing it is all the things being alive; for instance, there's the arch I've got to go through next walking about at the other end of the ground--and I should have croqueted the Queen's hedgehog just now, only it ran away when it saw mine coming!'\n",
      "    TARGET: — Non credo che giochino realmente, — disse Alice lagnandosi. — Litigano con tanto calore che non sentono neanche la loro voce... non hanno regole nel giuoco; e se le hanno, nessuno le osserva... E poi c'è una tal confusione con tutti questi oggetti vivi; che non c'è modo di raccapezzarsi.\n",
      " PREDICTED: — Non credo che si a dire a Alice , — e poi si a dire che si possono parlare di questo . E poi , se non si possono parlare di quelle umane sono ; e siccome non ci sono mai più , e non ci sono più che per altro che una delle ragazze non ci : ecco , la Regina che si la Regina ; e siccome le acque , e le di là sul tavolo per terra .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'My youngest,' replied the old man with a smile of affection.\n",
      "    TARGET: — L’ultimo — disse il vecchio con un sorriso carezzevole.\n",
      " PREDICTED: — Il vecchio — disse , sorridendo con un sorriso gioioso .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 11: 100% 4850/4850 [04:47<00:00, 16.84it/s, loss=3.047]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'It is strange, but there are!\n",
      "    TARGET: — È strano, ma ce n’è.\n",
      " PREDICTED: — È strano , ma là .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Levin got home just as the Princess arrived, and they met at the bedroom door.\n",
      "    TARGET: Quando Levin tornò a casa, s’incontrò con la principessa e si accostarono insieme alla porta della stanza da letto.\n",
      " PREDICTED: Levin entrò nella camera della principessa , e insieme si diresse nell ’ ingresso .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 12: 100% 4850/4850 [04:49<00:00, 16.77it/s, loss=2.977]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: But this mistake was not entirely caused by his omitting to consider that contingency, but also by the fact that, up to the day when he was face to face with his dying wife, he had not known his own heart.\n",
      "    TARGET: Ma l’errore suo era derivato non solo dal non aver supposto questa eventualità, ma anche dal non aver mai, fino a quel giorno dell’incontro con la moglie morente, conosciuto il proprio cuore.\n",
      " PREDICTED: Ma questo errore non era ancora in tutto il tempo di riflettere , ma anche , anche in quel giorno , si sforzava di di uscire , quando egli riconosceva la moglie non aveva il cuore , non lo aveva saputo .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'Come, Veslovsky! Tell us what keeps the bricks together!'\n",
      "    TARGET: — Sì, dite un po’, Veslovskij, con che cosa si uniscono le pietre?\n",
      " PREDICTED: — Andiamo , Veslovskij , ecco , su di noi , a .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 13: 100% 4850/4850 [04:52<00:00, 16.61it/s, loss=2.846]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Twenty years ago, a poor curate--never mind his name at this moment--fell in love with a rich man's daughter; she fell in love with him, and married him, against the advice of all her friends, who consequently disowned her immediately after the wedding.\n",
      "    TARGET: \"Vent'anni fa un povero pastore s'innamorò di una ricca ragazza; anche questa lo amava e lo sposò, nonostante i consigli dei parenti, che la rinnegarono.\n",
      " PREDICTED: — Tanto meglio , una povera creatura non può mai soffrire ; con una donna amata sarà giovane in buono una donna di amore ; ella si fece sentire con lui e con una donna che fa sempre , dopo aver sposato tutto il suo destino .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: With infinite difficulty, for he was stubborn as a stone, I persuaded him to make an exchange in favour of a sober black satin and pearl-grey silk.\n",
      "    TARGET: Dopo molto discutere, perché era inflessibile come un masso, si decise a prendere un vestito di raso nero e un altro di seta color perla.\n",
      " PREDICTED: Con poca fatica , per qualche cura di una religione , mi accorsi che la collera mi aveva data e mi in una mancanza di ricci bianca e raso nero un sol raso nero .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 14: 100% 4850/4850 [04:49<00:00, 16.74it/s, loss=2.822]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The little sail stood out against the purple sky, the gloaming lay around us, wrapping the world in rainbow shadows; and, behind us, crept the night.\n",
      "    TARGET: La piccola vela s’allargava contro il firmamento di porpora, il crepuscolo ci stava intorno, avvolgendo il mondo in ombre di arcobaleno; e dietro di noi strisciava la notte.\n",
      " PREDICTED: La vela stava in giro contro il cielo , e la pioggia apparve sul cielo che ci arrivava , e ci dietro la notte .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Have a bite of pie!'\n",
      "    TARGET: Mangerai il pirog!\n",
      " PREDICTED: il pasticcio di fiori !\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 15: 100% 4850/4850 [04:52<00:00, 16.56it/s, loss=2.180]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Not too much at first--restrain her,\" said the brother; \"she has had enough.\" And he withdrew the cup of milk and the plate of bread.\n",
      "    TARGET: — Non tanto alla volta, trattenetela, — disse il fratello. — Ora basta, — e allontanò la tazza col latte e il piatto del pane.\n",
      " PREDICTED: — Non sono troppo freddo , — disse il fratello — ha tanta pena che ha portato il latte e abbiamo portato il latte e abbiamo del latte .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: She seemed the emblem of my past life; and here I was now to array myself to meet, the dread, but adored, type of my unknown future day.\n",
      "    TARGET: Ella mi pareva l'emblema della mia vita passata, e colui dal quale presto stavo per andare, il tipo temuto ma adorato, della mia vita futura.\n",
      " PREDICTED: Pareva un ' infelice la mia vita , la mia vita , e giunsi a scendere le con la mia solita , ma la mia fantasia era piena d ' attesa .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 16: 100% 4850/4850 [04:47<00:00, 16.88it/s, loss=2.579]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I have not much pride under such circumstances: I would always rather be happy than dignified; and I ran after him--he stood at the foot of the stairs.\n",
      "    TARGET: In quella casa non avevo orgoglio. Gli corsi dietro e lo raggiunsi alla scala.\n",
      " PREDICTED: Non ho orgoglio di esser così felice : desideravo di esser felice e di vederlo in due , quando mi accorsi di esser veduta davanti alla scala .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Sir,\" I interrupted him, \"you are inexorable for that unfortunate lady: you speak of her with hate--with vindictive antipathy.\n",
      "    TARGET: — Signore, — dissi, — siete inesorabile per quella povera donna. Ne parlate con odio, con antipatia vendicativa.\n",
      " PREDICTED: — Signore , — interruppe , — siete venuta per la buona mamma ; voi dite con voi che vuole alla sua volontà .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 17: 100% 4850/4850 [04:46<00:00, 16.93it/s, loss=2.986]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I am sure most people would have thought him an ugly man; yet there was so much unconscious pride in his port; so much ease in his demeanour; such a look of complete indifference to his own external appearance; so haughty a reliance on the power of other qualities, intrinsic or adventitious, to atone for the lack of mere personal attractiveness, that, in looking at him, one inevitably shared the indifference, and, even in a blind, imperfect sense, put faith in the confidence.\n",
      "    TARGET: Quasi tutti lo avrebbero giudicato brutto, ma aveva nel portamento tanta fierezza naturale, tanta sicurezza nelle maniere, pareva curarsi tanto poco della mancanza di bellezza ed esser così intimamente convinto che le sue qualità compensavano largamente un'attrattiva tutta esteriore, che, guardandolo, si divideva la sua indifferenza o quasi quasi anche la sua convinzione.\n",
      " PREDICTED: Sono sicura che la gente avrebbe suscitato in lui un uomo , ma in lui era un orgoglio così delizioso , che il suo orgoglio . \" In questo modo così l ' indifferenza di una espressione , che la pittura ; le altre lo , alla svelta e facendo una condotta , per nascondere loro particolari , e per sempre più motivi di sentire una grande ammirazione .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Think of his misery; think of his danger--look at his state when left alone; remember his headlong nature; consider the recklessness following on despair--soothe him; save him; love him; tell him you love him and will be his.\n",
      "    TARGET: Rammentati com'è impetuoso di carattere, considera le conseguenze che può avere la disperazione: salvalo, amalo; digli che lo ami e che vuoi esser sua.\n",
      " PREDICTED: — Pensate alla sua bontà , il timore che la sua attrattiva era sola , ma la sua pazienza sarà sempre sola , l ' amore ; il dolore , lo .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 18: 100% 4850/4850 [04:47<00:00, 16.89it/s, loss=2.148]\n",
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: We were under the bridge, in exactly the same spot that we were when I began, and there were those two idiots, injuring themselves by violent laughing.\n",
      "    TARGET: Eravamo sotto la passerella, esattamente nel punto dove ci trovavamo quando avevo cominciato, e quei due idioti si sganasciavano dalle risa.\n",
      " PREDICTED: Eravamo sotto il ponte , come nel buio , in quel punto che ci saremmo , e , dopo , ci vollero fare due , .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: But in the corridor there was nobody, and in despair he returned and addressed Oblonsky, who was quietly smoking.\n",
      "    TARGET: Ma nel corridoio non compariva la persona ch’egli aspettava e, tornando indietro disperato e agitando le braccia, si rivolgeva a Stepan Arkad’ic che fumava tranquillamente.\n",
      " PREDICTED: Ma nel corridoio c ’ era un solo posto , e , ricevutane , si voltò a sedere verso Stepan Arkad ’ ic .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 19:  95% 4615/4850 [04:28<00:13, 16.98it/s, loss=2.067]"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    cfg = get_config()\n",
    "    cfg['batch_size'] = 6\n",
    "    cfg['preload'] = None\n",
    "    cfg['num_epochs'] = 20\n",
    "    train_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5818df9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
